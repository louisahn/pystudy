import pandas as pd
import numpy as np
import missingno as msno 

train.info()
train.describe()

#visualize missing values
%matplotlib inline
train = train.replace("nan", np.nan)
msno.matrix(train.sample(250))

#check null value (show True/False)
train.isnull().head()
train.isnull().sum()

#어느 데이터든 누락 데이터가 있으면 출력
train[train.isnull().any(axis=1)]

train.ticket.value_counts()

def data_fillna(dataset):
    # 중간값으로 누락된 나이 데이터를 채워준다.
    dataset['Age'].fillna(dataset['Age'].median(), inplace = True)

    # 최빈값(가장 자주 나오는 값)으로 누락된 승선위치를 채워준다.
    dataset['Embarked'].fillna(dataset['Embarked'].mode()[0], inplace = True)

    # 중간값으로 누락된 요금을 채워준다.
    dataset['Fare'].fillna(dataset['Fare'].median(), inplace = True)
    
    # 중간값으로 티켓 번호를 채워준다.
    dataset['TicketNumber'].fillna(dataset['TicketNumber'].median(), inplace=True)
    
data_fillna(train_mean)

# 중간값으로 채워주기 전 나이 데이터
train.hist(['Age'])
# 중간값으로 채워준 후의 나이 데이터
# 중간값이 눈에 띄게 늘어났다.
train_mean.hist(['Age'])

train_2 = train.copy()

train['Name'].str.split(", ", expand=True).head()
#0	Braund	Mr. Owen Harris
#1	Cumings	Mrs. John Bradley (Florence Briggs Thayer)

# 이름에서 Title 항목만 떼어낸다.
# 콤마로 분리를 하고 분리된 두 번째 데이터의 첫번째 항목을 가져온다.
train['Title'] = train['Name'].str.split(
    ", ", expand=True)[1].str.split(".", expand=True)[0]
train['Title'].head()

test['Title'] = test['Name'].str.split(
    ", ", expand=True)[1].str.split(".", expand=True)[0]
test['Title'].head()
#0     Mr
#1    Mrs

train.query("Title in ('Dona', 'Lady', 'the Countess')")

# 결혼한 여성 관련 호칭은 Mrs로 분류
train.loc[train['Title'].isin(['Dona', 'Lady', 'the Countess', 'Mme', 'Mlle']), 'Title'] = 'Ms'
train.loc[train['Title'].isin(['Don', 'Sir', 'Capt', 'Col', 'Major', 'Master', 'Rev', 'Dr', 'Jonkheer']), 'Title'] = 'Mr'

train.Age.isnull().sum()
train_groupby = train.copy()
train_groupby['Age'].fillna(train.groupby('Title')['Age'].transform('median'), inplace=True)

#	PassengerId	Survived	Pclass	Name	Sex	Age	SibSp	Parch	Ticket	Fare	Cabin	Embarked	Title
#0	1	0	3	Braund, Mr. Owen Harris	male	22.0	1	0	A/5 21171	7.2500	NaN	S	Mr
train_rf['FamilySize'] = train_rf['SibSp'] + train_rf['Parch'] + 1
print(train_rf['FamilySize'].value_counts())

train_rf.loc[train_rf['FamilySize'] == 1, 'FsizeD'] = 'singleton'
train_rf.loc[(train_rf['FamilySize'] > 1)  &  (train_rf['FamilySize'] < 5) , 'FsizeD'] = 'small'
train_rf.loc[train_rf['FamilySize'] > 4, 'FsizeD'] = 'large'
print(train_rf['FsizeD'].unique())
print(train_rf['FsizeD'].value_counts())

train_rf["NameLength"] = train["Name"].apply(lambda x: len(x))
bins = [0, 20, 40, 57, 85]
group_names = ['short', 'okay', 'good', 'long']
train_rf['NlengthD'] = pd.cut(train_rf['NameLength'], bins, labels=group_names)

print(train_rf["NlengthD"].unique())
#[okay, good, short, long]
#Categories (4, object): [short < okay < good < long]

train_rf['Deck'] = train_rf.Cabin.str[0]
train_rf['Deck'].unique() # 0 is for null values
#array([nan, 'C', 'E', 'G', 'D', 'A', 'B', 'F', 'T'], dtype=object)

def dummy_data(data, columns):
    for column in columns:
        data = pd.concat([data, pd.get_dummies(data[column], prefix = column)], axis=1)
        data = data.drop(column, axis=1)
    return data

def drop_data(data, columns):
    for column in columns:
        data = data.drop(column, axis=1)
    return data
    
dummy_columns = ['Sex', 'Pclass', 'Embarked', 'Title', 'FsizeD', 'NlengthD']
train_rf = dummy_data(train_rf, dummy_columns)
test_rf = dummy_data(test_rf, dummy_columns)
drop_columns = ['Name', 'Ticket', 'Fare', 'Cabin', 'Deck', 'Parch', 'SibSp', 'FamilySize']
train_rf = drop_data(train_rf, drop_columns)
train_rf = train_rf.drop('Survived', axis=1)
test_rf = drop_data(test_rf, drop_columns)


from sklearn.ensemble import RandomForestRegressor

def fill_missing_age(df):
    
    # Age값을 기준으로 학습과 테스트 세트로 분리
    train  = df.loc[ (df.Age.notnull()) ]# Age 값이 있는 row 
    test = df.loc[ (df.Age.isnull()) ]# Age 값이 없는 row 
    
    print(train.columns)
    # Age Label 데이터
    y = train['Age']
    
    # 피처를 생성해 준다. 전체 데이터 중 컬럼은 2번째 값 이후부터 가져옴
    train_X = train.values[:, 2::]
    test_X = test.values[:, 2::]
    
    print(train_X.shape)
    print(test_X.shape)
    print(len(y))
    
    # 모델을 학습시킨다.
    rtr = RandomForestRegressor(n_estimators=2000, n_jobs=-1)
    rtr.fit(train_X, y)
    
    # 학습한 데이터를 바탕으로 결측치를 예측한다.
    predictedAges = rtr.predict(test_X)
    
    # 예측한 값을 Age가 null 인 데이터에 채워준다. 
    df.loc[ (df.Age.isnull()), 'Age' ] = predictedAges 
    
    return df

train_rf = fill_missing_age(train_rf)
test_rf = fill_missing_age(test_rf)

test_rf['Fare'].fillna(test_rf['Fare'].median(), inplace = True)

from sklearn.preprocessing import MinMaxScaler

def normalize_value(data):
    scaler = MinMaxScaler()
    data["Age"] = scaler.fit_transform(data["Age"].values.reshape(-1,1))
    data["Fare"] = scaler.fit_transform(data["Fare"].values.reshape(-1,1))
    return data


train_data = normalize_value(train_rf)

#Predict with DT
from sklearn.tree import DecisionTreeClassifier

dt = DecisionTreeClassifier(max_depth=5, random_state=2018)
dt

X_train = train_data[feature_columns]
X_test = test_data[feature_columns]
y_label = train['Survived']

train_data.columns
feature_columns = ['Age', 'NameLength', 'Sex_female', 'Sex_male',
       'Pclass_1', 'Pclass_2', 'Pclass_3', 'Embarked_C', 'Embarked_Q', 'Embarked_S',
       'FsizeD_large', 'FsizeD_singleton', 'FsizeD_small', 'NlengthD_short',
       'NlengthD_okay', 'NlengthD_good', 'NlengthD_long']

dt.fit(X_train, y_label)

from sklearn.model_selection import KFold, cross_val_score
k_fold = KFold(n_splits=10, shuffle=True, random_state=2018)

scoring = 'accuracy'
dt_score = cross_val_score(dt, X_train, y_label, cv=k_fold, n_jobs=-1, scoring=scoring)
print(dt_score)

round(np.mean(dt_score)*100, 2)

prediction = dt.predict(X_test)
test['Survived'] = prediction
dt_submissions = test[['PassengerId', 'Survived']]
dt_submissions.head()


#Predict with RF
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import make_scorer, accuracy_score
from sklearn.model_selection import GridSearchCV

rfc = RandomForestClassifier()


parameters = {'n_estimators': [4, 6, 9, 15, 30], 
              'max_features': ['log2', 'sqrt','auto'], 
              'criterion': ['entropy', 'gini'],
              'max_depth': [2, 3, 5, 10], 
              'min_samples_split': [2, 3, 5],
              'min_samples_leaf': [1, 5, 8]
             }


acc_scorer = make_scorer(accuracy_score)
grid_obj = GridSearchCV(rfc, parameters, scoring=acc_scorer)
grid_obj

%time grid_obj = grid_obj.fit(X_train, y_label)

%time rfc = grid_obj.best_estimator_

%time rfc.fit(X_train, y_label)

# 학습한 데이터를 바탕으로 결측치를 예측한다.
%time rfc_predictions = rfc.predict(X_test)
test['Survived'] = rfc_predictions
rfc_submissions = test[['PassengerId', 'Survived']]
rfc_submissions.head()

scoring = 'accuracy'
rfc_score = cross_val_score(rfc, X_train, y_label, cv=k_fold, n_jobs=-1, scoring=scoring)
print(rfc_score)

rfc_score_mean = round(np.mean(rfc_score) * 100, 2)
rfc_score_mean
rfc_submissions.to_csv('submissions/submit_age_rf_rfc_{}.csv'.format(rfc_score_mean), index=False)

#XGB
import warnings
warnings.filterwarnings('ignore')

from xgboost import XGBClassifier
import xgboost as xgb
from sklearn.model_selection import RandomizedSearchCV


gbm_param_grid = {
    'n_estimators': range(7, 30),
    'max_depth': range(5, 10),
    'learning_rate': [.4, .45, .5, .55, .6],
    'colsample_bytree': [.6, .7, .8, .9, 1]
}

gbm = XGBClassifier()

xgb_random = RandomizedSearchCV(param_distributions=gbm_param_grid, 
                                estimator = gbm, scoring = "accuracy", 
                                verbose = 1, n_iter = 50, cv = k_fold)


xgb_random.fit(X_train, y_label)


xgb_best_score = round(xgb_random.best_score_ * 100, 2)
xgb_best_score

xgb_predictions = xgb_random.predict(X_test)
test['Survived'] = xgb_predictions
xgb_submissions = test[['PassengerId', 'Survived']]
xgb_submissions.head()


